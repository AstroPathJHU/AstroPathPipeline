{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to align two images with respect to each other.  We want to use a method similar to `skimage.feature.register_translation`, but we additionally want to get an uncertainty estimate on the alignment parameters.\n",
    "\n",
    "As a test case to determine the properties our uncertainty should have, we use a simple case of two rectangles.  To generate a rectangle we use this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rectangle(W, H, x1, y1, x2, y2):\n",
    "    assert 0 <= x1 < x2+1 <= W\n",
    "    assert 0 <= y1 < y2+1 <= H\n",
    "    result = np.zeros((H, W), dtype=float)\n",
    "    result[y1:y2+1,x1:x2+1] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or in math terms:\n",
    "$$R(x, y|x_1, y_1, x_2, y_2) = \\left\\{\n",
    "\\begin{array}{@{}ll@{}}\n",
    "  1, & \\text{if}\\ x_1<=x<=x_2\\ \\text{and}\\ y_1<=y<=y_2 \\\\\n",
    "  0, & \\text{otherwise}\n",
    "\\end{array}\\right.$$\n",
    "Also defining\n",
    "$$\\begin{align}w&=x_2-x_1 \\\\ h&=y_2-y_1\\end{align}$$\n",
    "\n",
    "To do the alignment, we maximize the cross-correlation of the function with itself.  The cross correlation is:\n",
    "$$C(d\\vec{x})=\\int_0^W dx \\int_0^H dy I_1(\\vec x) I_2(\\vec x+d\\vec x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosscorr(a, b):\n",
    "    A = np.fft.fft2(a)\n",
    "    B = np.fft.fft2(b)\n",
    "    C = A * np.conj(B)\n",
    "    c = np.real(np.fft.ifft2(C))\n",
    "    c = np.roll(c, c.shape[0]//2, axis=0)\n",
    "    c = np.roll(c, c.shape[1]//2, axis=1)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, let's say the images are two identical rectangles, and we'll also add a variable $A$ with units of intensity to allow for normalization:\n",
    "$$\n",
    "\\begin{align}\n",
    "I_1(x, y) &= I_2(x, y) = AR(x, y | x_1, y_1, x_2, y_2)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I1 = I2 = rectangle(100, 100, 40, 45, 60, 55)\n",
    "plt.imshow(I1); None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So then:\n",
    "$$\n",
    "C(d\\vec{x})=\\left\\{\n",
    "\\begin{array}{@{}ll@{}}\n",
    "A^2(w-|dx|)(h-|dy|), & \\text{if}\\ dx<w\\ \\text{and}\\ dy<h \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{array}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = crosscorr(I1, I2)\n",
    "plt.imshow(C, extent=[-50, 50, -50, 50]); None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the product of triangles in the $x$ and $y$ directions.  Note that we _can't_ use the width of this triangle as an error estimate.  The width of the triangle (however exactly you define that) is proportional to the widrh of the rectangle, so for example if we made a bigger rectangle along $x$, we get a wider triangle along x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = rectangle(100, 100, 20, 45, 80, 55)\n",
    "plt.imshow(tmp)\n",
    "plt.imshow(crosscorr(tmp, tmp), extent=[-50, 50, -50, 50]); None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't right: statistically, we expect $\\delta(dx)\\sim1/\\sqrt{w}$, $\\delta(dy)\\sim1/\\sqrt{h}$.  We'd also like the intensity to play in somehow: a more intense pixel provides a stronger constraint than a less intense one.\n",
    "\n",
    "The second derivatives of the function at the peak look useful.  For example\n",
    "$$\\frac{\\partial C}{\\partial x^\\pm}(0, 0)=\\mp A^2 h$$\n",
    "$$\\frac{\\partial^2C}{\\partial x^2}(0,0)=-2A^2h\\delta(x)$$\n",
    "The inverse square root of this is proportional to the things we want it to be proportional to and has units of $\\frac{1}{\\text{intensity}}$.  Hessian matrices often show up in the denominator of covariance matrices, so that's consistent with our general expectations.\n",
    "\n",
    "In the numerator, we need something with units of $\\text{intensity}\\cdot\\text{length}$.  The natural choice is $\\sqrt{\\delta C}$.\n",
    "\n",
    "So, at the end of the day our covariance matrix will be proportional to $$\\delta C\\left(\n",
    "\\begin{array}\n",
    "%\n",
    "\\frac{\\partial^2C}{\\partial x^2} &\n",
    "\\frac{\\partial^2C}{\\partial x \\partial y} \\\\\n",
    "\\frac{\\partial^2C}{\\partial x \\partial y} &\n",
    "\\frac{\\partial^2C}{\\partial y^2}\n",
    "\\end{array}\n",
    "\\right)^{-1}$$\n",
    "\n",
    "(Obviously we can't actually evaluate this in the case of rectangles because there's a delta function, but in a real case we can fit to a smooth function.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
